# Simple example of caret
## Syed Badruddoza

rm(list=ls())
require(caret)

# data creation
n<-1000; n_x<-12
xs<-matrix(rnorm(n*n_x,0,1),nrow=n,ncol=n_x)
beta<-rnorm(n_x,0,1)
p<-1/(1+exp(-(xs %*% beta) ))   # remember the logistic function?
dt<-data.frame(xs)
dt$y<-as.factor(round(p,0))
head(dt)

# set up 10-fold "cross validation" procedure
train_control <- trainControl(method = "cv", number = 10, savePredictions = "final")
# split sample into training and testing
intrain <- createDataPartition(dt$y,p=0.7,list=FALSE)
training <- dt[intrain,]
testing <- dt[-intrain,]
# define a grid of parameter options to try with ranger
rf_grid <- expand.grid(mtry = c(3:10),
                       splitrule = "gini",   #or variance
                       min.node.size = c(2,4,6,8))
rf_grid
#rf_grid fit the model with the parameter grid

x=dt[,-which(colnames(dt)=="y")]
y=dt$y
rf.model <- train(x = x,y = y,
                  method = "ranger",
                  trControl = train_control,
                  tuneGrid = rf_grid,
                  importance= "none") #or impurity_corrected, or permutation for continuous y
yhat_rf=predict(rf.model, newdata = dt)
yhat_rf[-intrain]
confusionMatrix(testing$y,yhat_rf[-intrain])

# define a grid of parameter options to try with xgboost # reusing train_control
xgb_grid <- expand.grid(nrounds = c(50,100,200,300), #max number of iteration
                        max_depth = c(1,5,10), #max depth of a tree
                        min_child_weight =1, #minimum sum of instance weight (hessian) needed in a child. Do not partition if weight<min_child_weight
                        subsample = 1, #for further subsamples
                        gamma = 0, #minimum loss reduction required to make a further partition on a leaf node of the tree, larger gamma means conservative algorithm
                        colsample_bytree = 0.8, #subsample ratio of columns when constructing each tree
                        eta = c(.01,.05,.1,.2) ) #step size of each boosting step
#xgb_grid fit the model with the parameter grid
xgboost.model <- train(y ~ ., 
                       training , 
                       method = "xgbTree", 
                       tuneGrid = xgb_grid,
                       trControl = train_control)
yhat_gb=predict(xgboost.model, newdata = dt)
confusionMatrix(testing$y,yhat_gb[-intrain])
